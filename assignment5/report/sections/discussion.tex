\input{tables/metrics_description}

As seen in section \ref{sec:results}, different algorithms obtained the best results according to the considered metrics described in table \ref{tab:metrics}.

For example in the karate network, Label propagation and Fast Greedy resulted in the best clustering according to both, the metrics as well as the closeness to the canonical clustering. On the other hand, for our DBLP generated network, Louvain generally resulted in the best metrics. Especially interesting is that the best metrics were not obtained by the canonical clustering. 

Blindly applying the metrics however hides some important characteristics of the cluster generated. The metrics Edges Inside, Average Degree, Expansion Conductance, Normalized Cut, Max ODF and Average ODF are biased towards clusterings with a fewer number of clusters. This is especially noteworthy in the Barabasi Albert networks where label propagation generated a single large cluster (check the plots in Appendix \ref{append:graphs}).

Thus judging the quality of a clustering solely on these metrics might not be sufficient. Depending on the use case, it might be desirable to generate clusterings that are close to the canonical clustering. The clusterings favored by the metrics rarely coincide with the actual canonical clustering and therefore don't seem to capture the property of real world clusters well.

In the networks we analyzed, we can't see a general trend on which algorithms are best at generating clusterings favoring certain metrics. This may however only reflect on the networks we analyzed and not on the clustering algorithms themselves. In the case of DBLP, our sampling method is a random walk which may impact the optimal clustering method.

Often, the best metrics obtained were the same for multiple algorithms. This might likewise be related to the networks we chose.

Another thing to consider is that all of the networks considered were either fully connected or filtered to be fully connected to apply all algorithms. % TODO Daniel please check if this is correct
Potentially partially disconnected networks might lead to different results. Likewise we only considered unweighted networks. Adding weights to the edges might similarly impact the results obtained.

Lastly, the Jaccard similarity gives a measure of the similarity of two different clusterings.

For ENRON and DBLP, we applied Label Propagation and Louvin respectively as the baseline. In the table \ref{tab:jaccard} it can be seen that the Jaccard similarity produced by the Louvain algorithm is significantly lower than 1 even for clusterings generated by the same method. Meaning, that the clusterings produced are quite different despite using the same algorithm. This shows that the clustering algorithms, Louvain, is quite unstable in it's output.

As we have seen in the previous labs, in many cases, the best algorithm to apply to a network heavily depends on what you want to discover or achieve. The data we have produced for this lab shows that this is the case as well when it comes to clustering algorithms and understanding your network structure and what properties your clustering should exhibit is essential for selecting the best clustering algorithm.
